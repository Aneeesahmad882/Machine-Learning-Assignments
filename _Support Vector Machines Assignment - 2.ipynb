{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f477879-b15e-4f90-8df0-03220959c2f4",
   "metadata": {},
   "source": [
    "# Answer1\n",
    "Polynomial functions and kernel functions are both concepts used in machine learning, particularly in the context of kernel methods, such as Support Vector Machines (SVMs). Let's discuss their relationship:\n",
    "\n",
    "### Polynomial Functions:\n",
    "\n",
    "A polynomial function is a mathematical function of the form:\n",
    "\n",
    "\\[ f(x) = a_nx^n + a_{n-1}x^{n-1} + \\ldots + a_1x + a_0 \\]\n",
    "\n",
    "In the context of machine learning, polynomial functions are often used as basis functions to transform input features. For example, given a feature \\( x \\), you might create polynomial features like \\( x^2 \\), \\( x^3 \\), and so on, to capture non-linear relationships in the data.\n",
    "\n",
    "### Kernel Functions:\n",
    "\n",
    "Kernel functions, on the other hand, play a crucial role in kernelized algorithms like Support Vector Machines (SVMs). These algorithms operate in a high-dimensional feature space implicitly defined by the kernel function. The kernel function computes the similarity (or inner product) between pairs of data points in this high-dimensional space.\n",
    "\n",
    "### Relationship:\n",
    "\n",
    "Polynomial kernel functions are a specific type of kernel function used in SVMs and other kernelized algorithms. The polynomial kernel is defined as:\n",
    "\n",
    "\\[ K(x, y) = (x \\cdot y + c)^d \\]\n",
    "\n",
    "where \\( x \\) and \\( y \\) are input feature vectors, \\( c \\) is a constant, and \\( d \\) is the degree of the polynomial.\n",
    "\n",
    "The polynomial kernel allows SVMs to implicitly operate in a high-dimensional space without explicitly computing the transformations of the input features. This is known as the \"kernel trick.\"\n",
    "\n",
    "In summary, polynomial functions are used to introduce non-linearities in the feature space, while polynomial kernel functions leverage these non-linearities in SVMs without explicitly computing the transformed features. The kernel functions, in general, provide a powerful way to capture complex relationships in the data without the need for explicit feature transformations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ff36861-3d12-4b3a-a574-e5fd7fb385b4",
   "metadata": {},
   "source": [
    "# Answer2\n",
    "\n",
    "Implementing an SVM with a polynomial kernel in Python using Scikit-learn is straightforward. Scikit-learn provides the SVC (Support Vector Classification) class, which allows you to use different kernel functions, including polynomial kernels. Here's an example of how you can implement an SVM with a polynomial kernel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ac5d842-000a-4b14-8f50-1dc52a3c37f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.00\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load a sample dataset (e.g., the Iris dataset)\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize the features (optional but recommended for SVMs)\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Create an SVM with a polynomial kernel\n",
    "# You can customize the degree (d), coefficient (coef0), and other parameters\n",
    "svm_poly = SVC(kernel='poly', degree=3, coef0=1, C=1.0)\n",
    "\n",
    "# Train the SVM\n",
    "svm_poly.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = svm_poly.predict(X_test)\n",
    "\n",
    "# Evaluate the accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "830c26dc-8f56-41de-b7a2-0b5ee75d8bf9",
   "metadata": {},
   "source": [
    "# Answer3\n",
    "In Support Vector Regression (SVR), the epsilon parameter (\\(\\varepsilon\\)) represents the width of the epsilon-insensitive tube around the predicted values. This tube determines the margin within which errors are tolerated and not penalized in the loss function.\n",
    "\n",
    "When you increase the value of epsilon in SVR, you are essentially widening the epsilon-insensitive tube. The impact of this on the number of support vectors can vary depending on the data distribution and the characteristics of the problem. Here are some general observations:\n",
    "\n",
    "1. **Wider Tolerance for Errors:**\n",
    "   - Larger epsilon allows for a wider margin of tolerance for errors. Data points within this margin are not considered errors and do not contribute to the loss function.\n",
    "   - As the tolerance for errors increases, the SVR model may be less sensitive to small fluctuations in the training data.\n",
    "\n",
    "2. **Fewer Support Vectors:**\n",
    "   - With a wider epsilon, fewer data points are treated as support vectors. Support vectors are the data points that lie on the boundaries of the epsilon-insensitive tube or are misclassified.\n",
    "   - The SVM algorithm aims to find a balance between minimizing the training error and maintaining a margin. As the tolerance for errors increases, the need for including more data points as support vectors diminishes.\n",
    "\n",
    "3. **Smoother Decision Boundary:**\n",
    "   - A larger epsilon often results in a smoother decision boundary or regression function. The model is more focused on capturing the general trend in the data and less concerned with fitting every data point precisely.\n",
    "\n",
    "It's important to note that the optimal choice for the epsilon parameter depends on the specific characteristics of your data and the goals of your regression task. A larger epsilon might be suitable when you want the model to be more robust to noise or when you are willing to accept a certain degree of error in predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43842d60-af22-4af9-9d19-62edeea2f90d",
   "metadata": {},
   "source": [
    "# Answer4\n",
    "Support Vector Regression (SVR) is a powerful regression technique that uses Support Vector Machines (SVMs) to model relationships between input features and target values. The performance of SVR is significantly influenced by the choice of kernel function and the values of key hyperparameters: C, epsilon (\\(\\varepsilon\\)), and gamma (\\(\\gamma\\)). Let's discuss each parameter and its impact on SVR performance:\n",
    "\n",
    "1. **Kernel Function:**\n",
    "   - **Explanation:** The kernel function determines the type of mapping that is applied to input features to transform them into a higher-dimensional space. Common choices include linear, polynomial, and radial basis function (RBF) kernels.\n",
    "   - **Impact:**\n",
    "      - **Linear Kernel (\\(K(x, y) = x^T \\cdot y\\)):** Suitable for linear relationships between features and targets.\n",
    "      - **Polynomial Kernel (\\(K(x, y) = (x \\cdot y + c)^d\\)):** Captures non-linear relationships, and the degree (\\(d\\)) parameter controls the degree of the polynomial.\n",
    "      - **RBF Kernel (\\(K(x, y) = \\exp(-\\gamma \\cdot \\|x-y\\|^2)\\)):** Effective for capturing complex non-linear patterns, and the gamma (\\(\\gamma\\)) parameter controls the kernel width.\n",
    "\n",
    "2. **C Parameter:**\n",
    "   - **Explanation:** The C parameter controls the trade-off between achieving a smooth fit and minimizing training errors. A smaller C allows for a smoother fit with a larger margin, while a larger C penalizes training errors more heavily.\n",
    "   - **Impact:**\n",
    "      - **Smaller C:** Results in a smoother model with a larger margin. Tolerates more errors in the training data.\n",
    "      - **Larger C:** Emphasizes fitting the training data more closely. May lead to a smaller margin and increased sensitivity to outliers.\n",
    "\n",
    "3. **Epsilon (\\(\\varepsilon\\)) Parameter:**\n",
    "   - **Explanation:** Epsilon defines the width of the epsilon-insensitive tube around the predicted values. It determines the range within which errors are not penalized in the loss function.\n",
    "   - **Impact:**\n",
    "      - **Smaller \\(\\varepsilon\\):** Requires the predicted values to be closer to the actual values. Results in a more strict model.\n",
    "      - **Larger \\(\\varepsilon\\):** Allows for a wider margin of tolerance for errors. Can lead to a more robust model, particularly in the presence of noise.\n",
    "\n",
    "4. **Gamma (\\(\\gamma\\)) Parameter:**\n",
    "   - **Explanation:** Gamma controls the width of the RBF kernel. A smaller gamma results in a wider kernel, and a larger gamma results in a narrower kernel.\n",
    "   - **Impact:**\n",
    "      - **Smaller \\(\\gamma\\):** Results in a wider kernel and a smoother decision boundary. May lead to underfitting.\n",
    "      - **Larger \\(\\gamma\\):** Leads to a narrower kernel and a more complex decision boundary. May lead to overfitting, particularly if not properly tuned.\n",
    "\n",
    "### Examples of Parameter Tuning:\n",
    "\n",
    "- **Scenario 1: Linear Relationship**\n",
    "  - **Kernel:** Linear\n",
    "  - **C:** Moderate value to balance fitting the data and avoiding overfitting.\n",
    "\n",
    "- **Scenario 2: Moderate Non-Linearity**\n",
    "  - **Kernel:** Polynomial with a moderate degree\n",
    "  - **C:** Moderate value for smoothness\n",
    "  - **Epsilon:** Adjust based on the desired tolerance for errors\n",
    "\n",
    "- **Scenario 3: Complex Non-Linearity**\n",
    "  - **Kernel:** RBF with an appropriate gamma\n",
    "  - **C:** Larger value for a tighter fit\n",
    "  - **Epsilon:** Adjust based on the desired tolerance for errors\n",
    "\n",
    "Remember, the optimal values depend on the specific characteristics of your data. It's advisable to perform hyperparameter tuning using techniques like cross-validation to find the best combination for your SVR model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "787172ec-066d-45fd-8a9b-02e47eac630d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'C': 1, 'epsilon': 0.5, 'gamma': 0.1, 'kernel': 'linear'}\n",
      "Mean Squared Error on Test Set: 2936.34\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "# Load the  dataset\n",
    "dataset = load_diabetes()\n",
    "X, y = dataset.data, dataset.target\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Create an SVR model\n",
    "svr = SVR()\n",
    "\n",
    "# Define the parameter grid for GridSearchCV\n",
    "param_grid = {\n",
    "    'kernel': ['linear', 'poly', 'rbf'],\n",
    "    'C': [0.1, 1, 10],\n",
    "    'epsilon': [0.1, 0.2, 0.5],\n",
    "    'gamma': [0.1, 0.01, 0.001]\n",
    "}\n",
    "\n",
    "# Perform GridSearchCV for hyperparameter tuning\n",
    "grid_search = GridSearchCV(svr, param_grid, scoring='neg_mean_squared_error', cv=5)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best parameters\n",
    "best_params = grid_search.best_params_\n",
    "print(\"Best Hyperparameters:\", best_params)\n",
    "\n",
    "# Train the SVR model with the best parameters\n",
    "best_svr = SVR(**best_params)\n",
    "best_svr.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = best_svr.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f\"Mean Squared Error on Test Set: {mse:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d30097f6-89c2-4d26-a235-bf38ffaf4dce",
   "metadata": {},
   "source": [
    "# Answer5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7098ae60-9170-4357-9139-856bb339d8ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.00\n",
      "Best Hyperparameters: {'C': 10, 'gamma': 'scale', 'kernel': 'linear'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['tuned_svc_classifier.joblib']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Import necessary libraries\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "import joblib\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Preprocess the data - Standard Scaling\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Create an instance of the SVC classifier\n",
    "svc = SVC()\n",
    "\n",
    "# Train the classifier on the training data\n",
    "svc.fit(X_train, y_train)\n",
    "\n",
    "# Use the trained classifier to predict labels of the testing data\n",
    "y_pred = svc.predict(X_test)\n",
    "\n",
    "# Evaluate the performance of the classifier using accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "# Tune hyperparameters using GridSearchCV\n",
    "param_grid = {'C': [0.1, 1, 10], 'kernel': ['linear', 'rbf', 'poly'], 'gamma': ['scale', 'auto']}\n",
    "grid_search = GridSearchCV(SVC(), param_grid, cv=5)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best parameters\n",
    "best_params = grid_search.best_params_\n",
    "print(\"Best Hyperparameters:\", best_params)\n",
    "\n",
    "# Train the tuned classifier on the entire dataset\n",
    "tuned_svc = SVC(**best_params)\n",
    "tuned_svc.fit(X_train,y_train)\n",
    "\n",
    "# Save the trained classifier to a file\n",
    "joblib.dump(tuned_svc, 'tuned_svc_classifier.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a86bb795-036d-4f1d-9056-11e08cf00ca3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
